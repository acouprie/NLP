{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: rake-nltk in /users/tweetcontextualization/acouprie/.local/lib/python3.6/site-packages (1.0.4)\n",
      "Requirement already satisfied: nltk in /users/tweetcontextualization/acouprie/.local/lib/python3.6/site-packages (from rake-nltk) (3.5)\n",
      "Requirement already satisfied: regex in /users/tweetcontextualization/acouprie/.local/lib/python3.6/site-packages (from nltk->rake-nltk) (2020.6.8)\n",
      "Requirement already satisfied: tqdm in /users/tweetcontextualization/acouprie/.local/lib/python3.6/site-packages (from nltk->rake-nltk) (4.46.1)\n",
      "Requirement already satisfied: joblib in /users/tweetcontextualization/acouprie/.local/lib/python3.6/site-packages (from nltk->rake-nltk) (0.15.1)\n",
      "Requirement already satisfied: click in /users/tweetcontextualization/acouprie/.local/lib/python3.6/site-packages (from nltk->rake-nltk) (7.1.2)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: python-rake in /users/tweetcontextualization/acouprie/.local/lib/python3.6/site-packages (1.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user rake-nltk\n",
    "!pip install --user python-rake\n",
    "\n",
    "from rake_nltk import Metric, Rake\n",
    "import RAKE\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "\n",
    "OUTPUT_FOLDER = '/projets/prevision/output/aggressive/'\n",
    "#OUTPUT_FOLDER = '/users/tweetcontextualization/acouprie/ukenvironmental/outputs/'\n",
    "# cmd and regex used: grep -Eo '[^.]* ([a-z]*|.)([t]|[T])error([a-z]*|.) [^.]*\\.' /projets/prevision/datasets/ukenvironmental/all_sentences.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_metric(metric):\n",
    "    result = \"\"\n",
    "    if(metric == \"degree\"):\n",
    "        result = \"Metric.WORD_DEGREE\"\n",
    "    elif(metric == \"frequency\"):\n",
    "        result = \"Metric.WORD_FREQUENCY\"\n",
    "    elif(metric == \"degree_to_frequency\"):\n",
    "        result = \"Metric.DEGREE_TO_FREQUENCY_RATIO\"\n",
    "    # default\n",
    "    else:\n",
    "        result = \"Metric.DEGREE_TO_FREQUENCY_RATIO\"\n",
    "    return result\n",
    "\n",
    "def rake_setup(file, length, metric):\n",
    "    #ignore = ['\\\"', '\\'', '://', '...']\n",
    "    r = Rake(stopwords=RAKE.SmartStopList(), max_length=length, ranking_metric=eval(define_metric(metric)))#, punctuations=ignore, ranking_metric=Metric.WORD_FREQUENCY) # Uses stopwords for english from NLTK, and all punctuation characters.\n",
    "    r.extract_keywords_from_text(file)\n",
    "    result = r.get_ranked_phrases_with_scores() # To get keyword phrases ranked highest to lowest.\n",
    "    return result\n",
    "    \n",
    "def rake(file, length, metric, output_file):\n",
    "    # compute rake\n",
    "    result = rake_setup(file, length, metric)\n",
    "    # write to file\n",
    "    write_rake_to_file(result, file, output_file)\n",
    "    \n",
    "def write_rake_to_file(result, input_file, output_file):\n",
    "    extract = {}\n",
    "    f = input_file.lower().replace(\" \", \"\")\n",
    "    for element in result:\n",
    "        occurence = len(f.split(element[1].replace(\" \", \"\")))-1\n",
    "        extract[element] = occurence\n",
    "    with open(output_file,\"w\") as output:\n",
    "        output.write(\"keyword,ngram,freq,rake\\n\")\n",
    "        for element in result:\n",
    "            output.write(\"{},{},{},{}\\n\".format(\n",
    "                element[1].translate(str.maketrans('', '', string.punctuation)),\n",
    "                len(element[1].split()),\n",
    "                extract[element],\n",
    "                element[0])\n",
    "            )\n",
    "\n",
    "freq_files = []\n",
    "def word_frequency(file, output_file):\n",
    "    freq_files.append(output_file)\n",
    "    r = Rake(stopwords=RAKE.SmartStopList())\n",
    "    r.extract_keywords_from_text(file)\n",
    "    freq = r.get_word_frequency_distribution()\n",
    "    with open(output_file,\"w\") as output:\n",
    "        output.write(\"keyword,freq\\n\")\n",
    "        for element in sorted(freq, key=freq.get, reverse=True):\n",
    "            output.write(\n",
    "                \"{},{}\\n\".format(\n",
    "                    element.replace(',', '').replace('\"', ''),\n",
    "                    freq[element]\n",
    "                )\n",
    "            )\n",
    "\n",
    "files_1_3_ngrams = []\n",
    "files_unigram = []\n",
    "def rake_extraction(input_file, output_name):\n",
    "    metrics = [\"degree\", \"frequency\", \"degree_to_frequency\"]\n",
    "    folder = OUTPUT_FOLDER + output_name + \"/\"\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    # word frequency\n",
    "    word_frequency(input_file, folder + \"frequency_\" + output_name + \".csv\")\n",
    "    # 1-3 ngrams\n",
    "    for i in range(0, len(metrics)):\n",
    "        rake_output = folder + output_name + \"_1-3ngrams_rake_\" + metrics[i] + \".csv\"\n",
    "        rake(input_file, 3, metrics[i], rake_output)\n",
    "        files_1_3_ngrams.append(rake_output)\n",
    "    # unigrams\n",
    "    rake_output = folder + output_name + \"_1ngrams_rake_\" + metrics[2] + \".csv\"\n",
    "    files_unigram.append(rake_output)\n",
    "    rake(input_file, 1, metrics[2], rake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/projets/prevision/datasets/aggressive/release-files/eng/trac2_eng_train.csv')\n",
    "AGGRESSIVE_DATASET = '/projets/prevision/datasets/aggressive/'\n",
    "aggressive_files = [\"cag_aggressive\", \"full_aggressive\", \"nag_aggressive\"]\n",
    "# extract data from csv to files\n",
    "for file in aggressive_files:\n",
    "    path = AGGRESSIVE_DATASET + file + \".txt\"\n",
    "    if os.path.isfile(path):\n",
    "        os.remove(path)\n",
    "\n",
    "# write data on different files\n",
    "full_file = open('/projets/prevision/datasets/aggressive/full_aggressive.txt',\"a\")\n",
    "cag_file = open('/projets/prevision/datasets/aggressive/cag_aggressive.txt',\"a\")\n",
    "nag_file = open('/projets/prevision/datasets/aggressive/nag_aggressive.txt',\"a\")\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    full_file.write(df['Text'].iloc[i])\n",
    "    if(df['Sub-task A'].iloc[i] == 'CAG'):\n",
    "        cag_file.write(df['Text'].iloc[i])\n",
    "    elif(df['Sub-task A'].iloc[i] == 'NAG'):\n",
    "        nag_file.write(df['Text'].iloc[i])\n",
    "\n",
    "full_file.close()\n",
    "cag_file.close()\n",
    "nag_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"cag_aggressive\", \"full_aggressive\", \"nag_aggressive\", \"terror\"]\n",
    "RESOURCE_FOLDER = '/projets/prevision/datasets/aggressive/'\n",
    "for folder in folders:\n",
    "    with open(RESOURCE_FOLDER + folder + \".txt\", 'r') as file:\n",
    "        rake_extraction(file.read(), folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_min_lines(files):\n",
    "    min_nb_lines = ['', 9999, 0]\n",
    "    i = 0\n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            nb_lines = len(lines)\n",
    "            if nb_lines < min_nb_lines[1]:\n",
    "                min_nb_lines = [i, nb_lines, lines[0].count(',')]\n",
    "            i = i + 1\n",
    "    return min_nb_lines\n",
    "\n",
    "def order_files(files):\n",
    "    mirror_files = []\n",
    "    for file in files:\n",
    "        mirror_files.append(file[::-1])\n",
    "    mirror_files = sorted(mirror_files)\n",
    "    result = []\n",
    "    for file in mirror_files:\n",
    "        result.append(file[::-1])\n",
    "    return result\n",
    "\n",
    "def merge_files(input_files, output_file):\n",
    "    files = order_files(input_files)\n",
    "    # define the max size of the file according to the smallest file to merge\n",
    "    smallest_file = define_min_lines(files)\n",
    "    path = OUTPUT_FOLDER + output_file\n",
    "    # delete output file if exist to overwrite\n",
    "    if os.path.isfile(path):\n",
    "        os.remove(path)\n",
    "    with open(path, 'a') as f:\n",
    "        # write header\n",
    "        for i in range(0, len(files)):\n",
    "            # get name from path\n",
    "            f.write(\"{},\".format(files[i].split('/')[-1].split('.')[0]))\n",
    "            for j in range(0,smallest_file[2]):\n",
    "                f.write(',')\n",
    "        f.write(\",\\n\")\n",
    "        # first file\n",
    "        with open(files[smallest_file[0]], 'r') as cf:\n",
    "            or_content = cf.readlines()\n",
    "        or_content = [x.strip() for x in or_content]\n",
    "        # extend lines to add columns\n",
    "        for i in range(0, len(files)):\n",
    "            if i == smallest_file[0]:\n",
    "                continue\n",
    "            with open(files[i], 'r') as cf:\n",
    "                content = cf.readlines()\n",
    "            content = [x.strip() for x in content]\n",
    "            for j in range(0, smallest_file[1]):\n",
    "                if i > smallest_file[0]:\n",
    "                    or_content[j] = or_content[j] + ',' + content[j]\n",
    "                else:\n",
    "                    or_content[j] = content[j] + ',' + or_content[j]\n",
    "        for j in range(0, smallest_file[1]):\n",
    "            f.write(or_content[j] + \"\\n\")\n",
    "\n",
    "merge_files(freq_files, \"all_frequencies.csv\")\n",
    "merge_files(files_1_3_ngrams, \"all_1_3_ngrams.csv\")\n",
    "merge_files(files_unigram, \"all_unigrams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
